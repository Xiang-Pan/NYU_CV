{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NwLQOFMMlev",
    "tags": []
   },
   "source": [
    "# Computer Vision CSCI-GA.2272-001 Assignment 1, part 1.\n",
    "\n",
    "Fall 2021 semester.\n",
    "\n",
    "Due date: **September 30th 2021.**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This assignment is an introduction to using PyTorch for training simple neural net models. Two different datasets will be used: \n",
    "- MNIST digits [handwritten digits]\n",
    "- CIFAR-10 [32x32 resolution color images of 10 object classes].\n",
    "\n",
    "## Requirements\n",
    "\n",
    "You should perform this assignment in PyTorch by modifying this ipython notebook (File-->Save a copy...).\n",
    "\n",
    "To install PyTorch, follow instructions at http://pytorch.org/\n",
    "\n",
    "Please submit your assignment by uploading this iPython notebook to Brightspace.\n",
    "\n",
    "## Warmup [5%]\n",
    "\n",
    "It is always good practice to visually inspect your data before trying to train a model, since it lets you check for problems and get a feel for the task at hand.\n",
    "\n",
    "MNIST is a dataset of 70,000 grayscale hand-written digits (0 through 9).\n",
    "60,000 of these are training images. 10,000 are a held out test set. \n",
    "\n",
    "CIFAR-10 is a dataset of 60,000 color images (32 by 32 resolution) across 10 classes\n",
    "(airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). \n",
    "The train/test split is 50k/10k.\n",
    "\n",
    "Use `matplotlib` and ipython notebook's visualization capabilities to display some of these images.\n",
    "[See this PyTorch tutorial page](http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) for hints on how to achieve this.\n",
    "\n",
    "** Relevant Cell: \"Data Loading\" **\n",
    "\n",
    "## Training a Single Layer Network on MNIST [10%]\n",
    "\n",
    "Start by running the training on MNIST.\n",
    "By default if you run this notebook successfully, it will train on MNIST.\n",
    "\n",
    "This will initialize a single layer model train it on the 60,000 MNIST training images for 10 epochs (passes through the training data). \n",
    "\n",
    "The loss function [cross_entropy](http://pytorch.org/docs/master/nn.html?highlight=cross_entropy#torch.nn.functional.cross_entropy) computes a Logarithm of the Softmax on the output of the neural network, and then computes the negative log-likelihood w.r.t. the given `target`.\n",
    "\n",
    "The default values for the learning rate, batch size and number of epochs are given in the \"options\" cell of this notebook. \n",
    "Unless otherwise specified, use the default values throughout this assignment. \n",
    "\n",
    "Note the decrease in training loss and corresponding decrease in validation errors.\n",
    "\n",
    "Paste the output into your report.\n",
    "(a): Add code to plot out the network weights as images (one for each output, of size 28 by 28) after the last epoch. Grab a screenshot of the figure and include it in your report. (Hint threads: [#1](https://discuss.pytorch.org/t/understanding-deep-network-visualize-weights/2060/2?u=smth) [#2](https://github.com/pytorch/vision#utils) )\n",
    "\n",
    "(b): Reduce the number of training examples to just 50. [Hint: limit the iterator in the `train` function]. \n",
    "Paste the output into your report and explain what is happening to the model.\n",
    "\n",
    "## Training a Multi-Layer Network on MNIST [10%]\n",
    "\n",
    "- Add an extra layer to the network with 1000 hidden units and a `tanh` non-linearity. [Hint: modify the `Net` class]. Train the model for 10 epochs and save the output into your report.\n",
    "-  Now set the learning rate to 10 and observe what happens during training. Save the output in your report and give a brief explanation\n",
    "\n",
    "## Training a Convolutional Network on CIFAR [25%]\n",
    "\n",
    "To change over to the CIFAR-10 dataset, change the `options` cell's `dataset` variable to `'cifar10'`.\n",
    "\n",
    "- Create a convolutional network with the following architecture:\n",
    "  - Convolution with 5 by 5 filters, 16 feature maps + Tanh nonlinearity.\n",
    "  - 2 by 2 max pooling (non-overlapping).\n",
    "  - Convolution with 5 by 5 filters, 128 feature maps + Tanh nonlinearity.\n",
    "  - 2 by 2 max pooling (non-overlapping).\n",
    "  - Flatten to vector.\n",
    "  - Linear layer with 64 hidden units + Tanh nonlinearity.\n",
    "  - Linear layer to 10 output units.\n",
    "\n",
    "Train it for 20 epochs on the CIFAR-10 training set and copy the output\n",
    "into your report, along with a image of the first layer filters.\n",
    "\n",
    "Hints: [Follow the first PyTorch tutorial](http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) or look at the [MNIST example](https://github.com/pytorch/examples/tree/master/mnist). Also, you may find training is faster if you use a GPU runtime (RunTime-->Change Runtime Type-->GPU). \n",
    "\n",
    "- Give a breakdown of the parameters within the above model, and the overall number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CrnqiscEKGWv"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V0NBWksLKRD7"
   },
   "outputs": [],
   "source": [
    "# options\n",
    "dataset = 'mnist' # options: 'mnist' | 'cifar10'\n",
    "batch_size = 64   # input batch size for training\n",
    "epochs = 10       # number of epochs to train\n",
    "lr = 0.01        # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "04539c4495c64d11b42905b3030fefff",
      "cd7fe7b4bb3f49029c2de890c3cd7d58",
      "f010b14902ec44c69648cabd2a0cd01d",
      "836e2b0833894246ac27f5f34e66ace1",
      "99e3ee30d7cc411bb7396ef9a96c875a",
      "0d4fa71e22304340b5639c8f88f74846",
      "bf7359f70e35462f877beb7d0935f8e3",
      "6d2f0036f8a34a5c8abccbcff73f01ae"
     ]
    },
    "id": "ab7pqvGUKVVf",
    "outputId": "c2009734-27ee-4191-9d6c-d189d9664329"
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "# Warning: this cell might take some time when you run it for the first time, \n",
    "#          because it will download the datasets from the internet\n",
    "dataset = \"mnist\"\n",
    "def get_dataloader(dataset = \"mnist\"):\n",
    "    if dataset == 'mnist':\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        trainset = datasets.MNIST(root='.', train=True, download=True, transform=data_transform)\n",
    "        testset = datasets.MNIST(root='.', train=False, download=True, transform=data_transform)\n",
    "\n",
    "    elif dataset == 'cifar10':\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        trainset = datasets.CIFAR10(root='.', train=True, download=True, transform=data_transform)\n",
    "        testset = datasets.CIFAR10(root='.', train=False, download=True, transform=data_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader  = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Visulization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image index is within trainset\n",
    "image_index = 0\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "trainset = datasets.MNIST(root='.', train=True, download=True, transform=data_transform)\n",
    "\n",
    "image, label = trainset[image_index]\n",
    "image = image.reshape((28, 28))\n",
    "\n",
    "# # Plot\n",
    "plt.title('Label is {label}'.format(label=label))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cifar10 visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc5ca448820>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# image index is within trainset\n",
    "image_index = 1\n",
    "\n",
    "trainset = datasets.CIFAR10(root='.', train=True, download=True, transform=None)\n",
    "image, label = trainset[image_index]\n",
    "\n",
    "# # Plot\n",
    "plt.title('Label is {label}'.format(label=label))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Single Layer Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mRkBGjAEKZM0"
   },
   "outputs": [],
   "source": [
    "## network and optimizer\n",
    "if dataset == 'mnist':\n",
    "    num_inputs = 784\n",
    "elif dataset == 'cifar10':\n",
    "    num_inputs = 3072\n",
    "\n",
    "num_outputs = 10 # same for both CIFAR10 and MNIST, both have 10 classes as outputs\n",
    "#   - Convolution with 5 by 5 filters, 16 feature maps + Tanh nonlinearity.\n",
    "#   - 2 by 2 max pooling (non-overlapping).\n",
    "#   - Convolution with 5 by 5 filters, 128 feature maps + Tanh nonlinearity.\n",
    "#   - 2 by 2 max pooling (non-overlapping).\n",
    "#   - Flatten to vector.\n",
    "#   - Linear layer with 64 hidden units + Tanh nonlinearity.\n",
    "#   - Linear layer to 10 output units.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, num_inputs) # reshape input to batch x num_inputs\n",
    "        output = self.linear(input)\n",
    "        return output\n",
    "\n",
    "network = Net(num_inputs, num_outputs)\n",
    "optimizer = optim.SGD(network.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3DfrCg4-Kfq6"
   },
   "outputs": [],
   "source": [
    "def train(epoch, train_loader):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(test_loader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        #data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = network(data)\n",
    "        test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "        #test_loss += F.cross_entropy(output, target, sum=True).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Weight Visulization\n",
    "(a): Add code to plot out the network weights as images (one for each output, of size 28 by 28) after the last epoch. Grab a screenshot of the figure and include it in your report. (Hint threads: #1 #2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'train_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_211032/1044825472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'train_loader'"
     ]
    }
   ],
   "source": [
    "rain_loader, test_loader \n",
    "for epoch in range(50):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'visdom' has no attribute 'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106898/2066205028.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisdom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'visdom' has no attribute 'image'"
     ]
    }
   ],
   "source": [
    "import visdom\n",
    "visdom.image(network.linear.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train 50 Epoch\n",
    "(b): Reduce the number of training examples to just 50. [Hint: limit the iterator in the train function]. Paste the output into your report and explain what is happening to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "collapsed": true,
    "id": "c52XmoeCKja2",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bc492b48-2451-48c0-a1a5-f4220dc6ad88",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.303359\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.478013\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.436842\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.341980\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.373091\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.212815\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.346881\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.213501\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.361849\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.331368\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.479575\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.311251\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.431315\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.331584\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.463623\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.490695\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.203455\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.350757\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.382515\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.283072\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.156617\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.369936\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.273166\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.132814\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.336206\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.201013\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.430146\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.230611\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.232578\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.318143\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.565386\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.237202\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.648895\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.248712\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.357200\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.355044\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.434139\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.329410\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.352406\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.291272\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.382163\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.160828\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.285728\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.211620\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.255243\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.143264\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.316421\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.502482\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.284712\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.393001\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.364041\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.578771\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.153240\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.241250\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.215249\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.312826\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.166869\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.338002\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.147311\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.173128\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.408466\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.499950\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.384036\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.343373\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.411051\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.305980\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.228570\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.230951\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.301853\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.228187\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.269293\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.451446\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.274004\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.247463\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.223991\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.115703\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.380994\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.296792\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.328150\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.151231\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.210538\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.151457\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.381801\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.466962\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.320358\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.338241\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.240324\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.142729\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.424282\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.363835\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.215775\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.274167\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.264990\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.347111\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.316232\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.421595\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.161379\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.285072\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.372270\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.424708\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.187235\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.417081\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.230168\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.237390\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.252304\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.230296\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.399311\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.347140\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.225584\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.252805\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.529039\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.313593\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.512486\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.182934\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.199044\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.266662\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.206616\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.362858\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.286547\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.319038\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.136101\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.268699\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.318562\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.433342\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.159699\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.118620\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.167419\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.267321\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.214341\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.505283\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.230752\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.157206\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.294683\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.329011\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.277994\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.224113\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.351579\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.204253\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.293110\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.151336\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.276378\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.280942\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.228989\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.196710\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.443625\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.160036\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.135888\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.150503\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.348382\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.334965\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.270741\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.228041\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.176085\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.085809\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.427029\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.221868\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.082044\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.253407\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.424038\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.252404\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.172467\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.103600\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.330691\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.541165\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.236454\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.428331\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.233439\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.685820\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.164776\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.347751\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.410794\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.276307\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.363742\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.334626\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.210289\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.401659\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.350846\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.157466\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.164452\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.497385\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.124096\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.201625\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.192665\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.191342\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.155299\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.171763\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.332240\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.107291\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.180687\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.334158\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.407658\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.289102\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.276267\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106898/1044825472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_106898/1610122175.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "4Nfl06zkKmfX",
    "outputId": "836b528e-5631-445d-ed27-2bd3cea703d5"
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training a Multi-Layer Network on MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (in,1000) (1000,10)\n",
    "- Add an extra layer to the network with 1000 hidden units and a `tanh` non-linearity. [Hint: modify the `Net` class]. Train the model for 10 epochs and save the output into your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential( nn.Linear(num_inputs, 1000), nn.Linear(1000, num_outputs))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, num_inputs) # reshape input to batch x num_inputs\n",
    "        output = self.net(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(num_inputs, num_outputs)\n",
    "optimizer = optim.SGD(network.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.351991\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.554521\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.382930\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.474488\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.361806\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.426693\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.370396\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.257200\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.370053\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.327832\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.288343\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.286894\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.320418\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.272572\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.599799\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.429267\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.322669\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.432354\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.383577\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.312381\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.152348\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.159845\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.314319\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.588592\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.387761\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.464457\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.329442\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.424994\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.180424\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.369272\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.285433\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.247151\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.600679\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.344457\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.297726\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.324363\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.357890\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.239009\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.177075\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.339028\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.308916\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.474359\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.280574\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.125540\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.219298\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.279016\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.159842\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.384604\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.189877\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.188955\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.254517\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.177626\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.383720\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.414260\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.175302\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.242784\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.214090\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.555769\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.357591\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.417805\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.346378\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.386949\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.321321\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.213287\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.313440\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.233007\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.479717\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.269013\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.322001\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.351012\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.141473\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.280439\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.338060\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.147256\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.336913\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.118866\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.166901\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.464016\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.268475\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.245336\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.281970\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.144259\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.264338\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.309453\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.335262\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.231353\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.354064\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.230644\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.216573\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.207726\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.256513\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.241204\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.253740\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.221951\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.300950\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.416597\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.403012\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.178726\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.342465\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.318209\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.340952\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.156645\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.343323\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.205053\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.256160\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.242676\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.308871\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.339431\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.165564\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.096052\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.214393\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.238879\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.104141\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.132742\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.264465\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.192089\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.235109\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.192297\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.212261\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.264172\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.277374\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.523438\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.338293\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.169682\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.343900\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.259269\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.382971\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.207170\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.313650\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.509662\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.232701\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.376097\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.201009\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.384910\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.320745\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.438424\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.370941\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.145512\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.246146\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.151665\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.171421\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.466409\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.428146\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.276110\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.206541\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.351701\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.124030\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.241120\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.348825\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.094172\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.302842\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.297278\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.332918\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.170668\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.306556\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.443736\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.444196\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.162938\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.224363\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.158631\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.295416\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.101548\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.165688\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.224230\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.266725\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.300162\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.227169\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.334011\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.345011\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.065785\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.285518\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.299363\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.269967\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.320670\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.306103\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.161106\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.299496\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.554554\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.220206\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.277121\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.160564\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.307214\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.058973\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.214352\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.182465\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.722234\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.438971\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.240926\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.172238\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.076189\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.210668\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.332938\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.151255\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.192661\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.228567\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.411130\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.169934\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.475502\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.262183\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.118891\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.284714\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.188831\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.225248\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.298137\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.223574\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.293146\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.517044\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.192842\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.132341\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.348774\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.221433\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 0.223016\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.233409\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 0.172455\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.352068\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 0.402355\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.237206\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 0.273721\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.099963\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 0.098940\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.211484\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 0.424882\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.113413\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 0.085914\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.334908\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 0.398187\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.154297\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 0.253466\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.267134\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 0.351477\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.091961\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 0.167104\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.323139\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 0.307098\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.229125\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 0.186415\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.239739\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 0.241738\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.262730\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 0.283021\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.249424\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 0.228398\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.170524\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 0.321074\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.235691\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 0.197920\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.102064\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 0.375404\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.275444\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 0.340783\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.141597\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 0.269926\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.345891\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 0.255534\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.234319\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 0.259690\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.140932\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 0.436851\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.093364\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 0.144375\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.218876\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 0.100957\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.138159\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 0.267799\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.183213\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 0.288636\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.320105\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 0.102348\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.236431\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 0.364611\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.226382\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 0.154732\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.096413\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 0.447276\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.228772\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 0.121030\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.182122\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 0.115738\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.158925\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 0.176234\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.172912\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 0.134070\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.202079\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 0.362148\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.213970\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 0.184529\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.210351\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 0.133997\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.399257\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 0.198721\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.201481\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 0.234254\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.094107\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 0.116034\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.391371\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 0.183863\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.149997\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 0.114511\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.281826\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 0.225634\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.132776\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 0.313813\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.139616\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 0.081247\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.578366\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 0.213451\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.318154\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 0.179429\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.311296\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 0.124667\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.095030\n",
      "Train Epoch: 31 [6400/60000 (11%)]\tLoss: 0.352308\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.240264\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 0.164324\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.123756\n",
      "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 0.352695\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.148194\n",
      "Train Epoch: 31 [44800/60000 (75%)]\tLoss: 0.295110\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.231772\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 0.289186\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.393812\n",
      "Train Epoch: 32 [6400/60000 (11%)]\tLoss: 0.499678\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.340732\n",
      "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 0.118787\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.212415\n",
      "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 0.207896\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.446813\n",
      "Train Epoch: 32 [44800/60000 (75%)]\tLoss: 0.251867\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.281803\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 0.208516\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.165888\n",
      "Train Epoch: 33 [6400/60000 (11%)]\tLoss: 0.217292\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.282778\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 0.331582\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.137311\n",
      "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 0.406309\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.331272\n",
      "Train Epoch: 33 [44800/60000 (75%)]\tLoss: 0.437963\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.174156\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 0.205006\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.112919\n",
      "Train Epoch: 34 [6400/60000 (11%)]\tLoss: 0.172458\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.126567\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 0.551634\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.243789\n",
      "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 0.126088\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.418126\n",
      "Train Epoch: 34 [44800/60000 (75%)]\tLoss: 0.376440\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.130604\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 0.449489\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.160448\n",
      "Train Epoch: 35 [6400/60000 (11%)]\tLoss: 0.398995\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.380947\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 0.260018\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.163237\n",
      "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 0.202775\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.239275\n",
      "Train Epoch: 35 [44800/60000 (75%)]\tLoss: 0.119378\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.136451\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 0.181044\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.326845\n",
      "Train Epoch: 36 [6400/60000 (11%)]\tLoss: 0.108145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106898/1044825472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_106898/1610122175.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pl/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;31m# PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning_rata = 10\n",
    "-  Now set the learning rate to 10 and observe what happens during training. Save the output in your report and give a brief explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(num_inputs, num_outputs)\n",
    "optimizer = optim.SGD(network.parameters(), lr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Convolutional Network on CIFAR\n",
    "To change over to the CIFAR-10 dataset, change the `options` cell's `dataset` variable to `'cifar10'`.\n",
    "\n",
    "- Create a convolutional network with the following architecture:\n",
    "  - Convolution with 5 by 5 filters, 16 feature maps + Tanh nonlinearity.\n",
    "  - 2 by 2 max pooling (non-overlapping).\n",
    "  - Convolution with 5 by 5 filters, 128 feature maps + Tanh nonlinearity.\n",
    "  - 2 by 2 max pooling (non-overlapping).\n",
    "  - Flatten to vector.\n",
    "  - Linear layer with 64 hidden units + Tanh nonlinearity.\n",
    "  - Linear layer to 10 output units.\n",
    "\n",
    "Train it for 20 epochs on the CIFAR-10 training set and copy the output\n",
    "into your report, along with a image of the first layer filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(CNN, self).__init__()\n",
    "        self.net = nn.Sequential( \n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(5,5)),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(5,5)),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Flatten(1,-1),\n",
    "            nn.Linear(75,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.net(input)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "network = CNN(num_inputs, num_outputs)\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.01)\n",
    "train_loader, test_loader = get_dataloader(\"cifar10\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1.1 - Computer Vision CSCI-GA.2272-001.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04539c4495c64d11b42905b3030fefff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f010b14902ec44c69648cabd2a0cd01d",
       "IPY_MODEL_836e2b0833894246ac27f5f34e66ace1"
      ],
      "layout": "IPY_MODEL_cd7fe7b4bb3f49029c2de890c3cd7d58"
     }
    },
    "0d4fa71e22304340b5639c8f88f74846": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d2f0036f8a34a5c8abccbcff73f01ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "836e2b0833894246ac27f5f34e66ace1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d2f0036f8a34a5c8abccbcff73f01ae",
      "placeholder": "",
      "style": "IPY_MODEL_bf7359f70e35462f877beb7d0935f8e3",
      "value": " 170500096/? [00:20&lt;00:00, 77534659.00it/s]"
     }
    },
    "99e3ee30d7cc411bb7396ef9a96c875a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bf7359f70e35462f877beb7d0935f8e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd7fe7b4bb3f49029c2de890c3cd7d58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f010b14902ec44c69648cabd2a0cd01d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d4fa71e22304340b5639c8f88f74846",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_99e3ee30d7cc411bb7396ef9a96c875a",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
